{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7f1df4",
   "metadata": {},
   "source": [
    "# Grade: 100 points\n",
    "\n",
    "# Assignment 01: Traditional Machine Learning \n",
    "\n",
    "## Instructions\n",
    "\n",
    "#### Follow These Steps before submitting your assignment \n",
    "\n",
    "This notebook contains the questions for Assignment 1. \n",
    "\n",
    "You must upload this completed Jupyter Notebook file as your submission (other file types are not permitted and will result in a grade of 0).***\n",
    "\n",
    "* If you have trouble running neural network models on your laptop, you can use online platforms, like **[Google Colab](https://colab.research.google.com/)**.\n",
    "* All Figures should have a x- and y-axis label and an appropriate title.\n",
    "**Ensure that your code runs correctly by choosing \"Kernel -> Restart and Cell -> Run All\" before submitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd59110",
   "metadata": {},
   "source": [
    "# Datasets:\n",
    "\n",
    "`Dataset1.csv` lists the housing market data, where the goal is to predict house prices based on various factors:\n",
    "\n",
    "- SqFt: Square footage of the house\n",
    "- Bedrooms: Number of bedrooms\n",
    "- Bathrooms: Number of bathrooms\n",
    "- Dist_Center: Distance to city center (in miles)\n",
    "- House_Age: Age of the house (years)\n",
    "- Floors: Number of floors\n",
    "- Lot_Size: Lot size (square feet)\n",
    "- Walk_Score: Walkability score (0-100)\n",
    "- HOA_Fee: Monthly HOA fee (if applicable)\n",
    "- Crime_Rate: Local crime rate (incidents per 1000 residents)\n",
    "- Dist_School: Distance to the nearest school (miles)\n",
    "- Grocery_Stores: Number of nearby grocery stores\n",
    "- Prop_Tax: Property tax rate (%)\n",
    "- Maint_Cost: Numerical\tYearly maintenance costs \n",
    "- Median_Income: Household income of the neighborhood (median)\n",
    "- Region: Region (values: 'A', 'B', 'C', 'D', 'E')\n",
    "- House_Cond: House condition ('Low', 'Medium', 'High')\n",
    "- Urban_Rural: Urban/Rural location ('Urban', 'Rural')\n",
    "- House_Price: House price (Target variable)\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "`Dataset2.csv` is a loan status dataset to predict whether a loan application will be fully approved, conditionally approved, or rejected based on various factors:\n",
    "\n",
    "- Credit_Score: Applicant's credit score (300-850, higher is better)\n",
    "- Income: Monthly income\n",
    "- Loan_Amount: Requested loan amount \n",
    "- Loan_Term: Duration of loan (in months)\n",
    "- Debt_Income: Debt-to-income ratio (%)\n",
    "- Open_Accounts: Number of active credit accounts\n",
    "- Hist_Length: Credit history length (years)\n",
    "- Delinquencies: Number of past missed payments\n",
    "- Total_Loan_Balance: Total outstanding loan balance\n",
    "- Credit_Inquiries: Number of recent hard credit inquiries\n",
    "- Employer_Tenure: How long the applicant has been at their job (years)\n",
    "- LTV_Ratio: Loan-to-value ratio (%)\n",
    "- Loan_Purpose: Purpose of the loan (X, Y, Z)\n",
    "- Employment_Type: Employer size (Small, Medium, Large)\n",
    "- Loan_Status: Target Variable(0 is Loan Rejected, 1 is Conditionally Approved, 2 is Fully Approved)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ccc9b9022749034",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d192c12c",
   "metadata": {},
   "source": [
    "# Q1 - Data Loading and Exploration (5 pts)\n",
    "\n",
    "1. Load the Dataset1.\n",
    "2. Display basic statistics and inspect for missing data.\n",
    "3. Encode the categorical features (one-hot encoding). \n",
    "4. Visualize the distribution of all features using histogram.\n",
    "5. **Discussion Question:** Why is it important to explore and visualize the data before building any models? What types of trends or problems could you uncover at this stage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c278db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Dataset1\n",
    "data = pd.read_csv('Dataset1.csv')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Display basic statistics\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e4978d2d711b475",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5bc4d623325d570",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 2. inspect for missing data\n",
    "df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eae90330fc55c3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 3. Encode the categorical features (one-hot encoding)\n",
    "categorical_cols = ['Region', 'House_Cond', 'Urban_Rural']\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "encoded_array = encoder.fit_transform(df[categorical_cols])\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Drop original categorical columns and merge\n",
    "df_encoded = df.drop(columns=categorical_cols).reset_index(drop=True)\n",
    "df_encoded = pd.concat([df_encoded, encoded_df], axis=1)\n",
    "df_encoded.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82895bf654d838b1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 4. Visualize the distribution of all features using histogram.\n",
    "\n",
    "# Categorical Features\n",
    "for column in categorical_cols:\n",
    "    plt.hist(df[column])\n",
    "    plt.title(f\"Distribution of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36ebcf467889abf8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Numerical Features\n",
    "numerical_cols = [column for column in df.columns if column not in categorical_cols]\n",
    "for column in numerical_cols:\n",
    "    plt.hist(df_encoded[column])\n",
    "    plt.title(f\"Distribution of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1e3e9ab2f0b69f4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "32f440c6",
   "metadata": {},
   "source": [
    "**Answer to Discussion Question**: \n",
    "1. Detect missing values, outliers in the dataset, so we can conduct proper data cleaning.\n",
    "2. Show feature distributions and feature types in the dataset, so we can conduct proper feature engineering.\n",
    "3. Also, we can predict what models will work for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dd942",
   "metadata": {},
   "source": [
    "# Q2 - Outlier Detection (10 pts)\n",
    "1. Train a Gaussian Mixture Model (GMM) on Dataset1 to identify potential outliers.\n",
    "\n",
    "2. Remove the detected outliers and save the cleaned dataset.\n",
    "\n",
    "3. How many outliers you detected?\n",
    "\n",
    "4. Visualize the histogram plot of the remaining data.\n",
    "\n",
    "5. **Discussion Question**: What are outliers? and why it is important to detect them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ee547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97e12a5d",
   "metadata": {},
   "source": [
    "**Answer to Discussion Question**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f596e02",
   "metadata": {},
   "source": [
    "# Q3 - Correlation and Feature Selection (10 pts)\n",
    "\n",
    "1. Visualize correlations between features and target using a heatmap to identify highly correlated features.\n",
    "2. Select numerical features with correlation above two certain thresholds (0.02 and 0.04), and print them.\n",
    "3. **Discussion Question:** How do you interpret a correlation value? Does a higher correlation always mean a feature is more important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e7a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea694490",
   "metadata": {},
   "source": [
    "**Answer to Discussion Question**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689f657",
   "metadata": {},
   "source": [
    "# Q4 - Multiple Linear Regression (20 pts)\n",
    "\n",
    "1. Build three different subsets of our data using two different sets of features based on correlation thresholds in previous question, as well as the original dataset with all features. For each subset, split the data into train and test and hold out 30% of observations as the test set. Pass random_state=42 to train_test_split to ensure you get the same train and tests sets as the solution and normalize (z-normalization) the data splits. \n",
    "2. Build two different multiple linear regression models using the subsets made by two different thresholds in Q3, and train them on their normalized training sets. \n",
    "3. Now build and fit a Lasso Regression model to the training data using all features in the dataset. The penalization parameter is set to 0.5.\n",
    "4. Evaluate the three models on their test sets and compare the models using R² and RMSE.\n",
    "5. **Discussion Question:** How do we decide which features to include in a multiple linear regression model? What challenges might arise from using too many features?\n",
    "6. **Discussion Question:** Among the three models, which model performs the best and why?\n",
    "7. **Discussion Question:** If a model has a high R² value but a large RMSE, what might that indicate about the model's performance?\n",
    "8. **Discussion Question:** Discuss next steps for potential improvements to the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2591cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01ad85d4",
   "metadata": {},
   "source": [
    "**Answer to the first Discussion Question**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964b50a",
   "metadata": {},
   "source": [
    "**Answer to the socond Discussion Question**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705dc1b",
   "metadata": {},
   "source": [
    "**Answer to the third Discussion Question**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead441f9",
   "metadata": {},
   "source": [
    "**Answer to the fourth Discussion Question**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b0b32",
   "metadata": {},
   "source": [
    "# Q 5 - Data Loading and Classification (25 pts)\n",
    "\n",
    "1. Load the Dataset2.\n",
    "2. Display basic statistics and inspect for missing data.\n",
    "3. Encode the categorical features (one-hot encoding). \n",
    "4. Split the data into train, validation and test and hold out 20% and 10% of observations as the validation and test set, respectively. Pass random_state=42.\n",
    "5. Normalize the data (Z-normalization) and fit the following models to the training samples:\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbors (KNN) with K equals to 3\n",
    "- Random Forest (RF) that consists of 5 base decision trees with the maximum depth of 5\n",
    "- Single-Layer Neural Network (Perceptron) with stochastic gradient descent (SGD) optimizer and a learning rate of 0.1, run the model for 10 iterations/epochs.\n",
    "\n",
    "6. Report the training time in milli second for all models. \n",
    "\n",
    "7. Use the Random Forest model you built to generate feature importance scores and a horizontal bar chart to plot the importance scores of all features in descending order. \n",
    "\n",
    "8. Select the important features from most to least important until the accumulated relative importance score reaches 90% or 0.9 and print out the selected features with their importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9b7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "463a365b",
   "metadata": {},
   "source": [
    "# Q 6 - Model Selection (10 pts)\n",
    "\n",
    "1. Report the prediction results of all models in Q5 on the test set of Dataset2, using these evaluation metrics: Confusion matrix, F1-score, Recall, Precision and Accuracy. \n",
    "2. Plot the ROC curve and report AUC of the predictions on the test set.\n",
    "3. Report the test time (in milli second) for all models. \n",
    "4. **Discussion Question:** Why is AUC-ROC a better metric than accuracy for this datasets? Provide an example where accuracy can be misleading.\n",
    "5. **Discussion Question:** Among all models, wich one you would choose? why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1606658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f9e4ee",
   "metadata": {},
   "source": [
    "**Answer to the first Discussion Question:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b3c4a",
   "metadata": {},
   "source": [
    "**Answer to the second Discussion Question:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bd1824",
   "metadata": {},
   "source": [
    "# Q 7 - Model Selection (20 pts)\n",
    "\n",
    "1. Build a Multi-Layer Perceptron (MLP) and fit it to the normalized training set of Dataset2- The details of the MLP are as follows:\n",
    "   * Two hidden layers (H1, H2), with 50 and 100 neurons/units in H1 and H2, respectively. \n",
    "   * Use tanh function as the activation function for hidden layers.\n",
    "   * Use a proper acitivation function for the output layer.  \n",
    "   * Use Stochastic gradient descent optimizer with a learning rate of 0.1.\n",
    "   * Run the model for 10 iterations/epochs \n",
    "   \n",
    "2.  Report the training time in milli second\n",
    "3.  Record the validation and training loss for each iteration, and make the plot of learning curves (iterations/epochs vs loss).\n",
    "4.  Report the prediction results of MLP on the test set of Dataset2, using these evaluation metrics: Confusion matrix, F1-score, Recall, Precision and Accuracy.\n",
    "5.  Report the test time (in milli second) for MLP. \n",
    "6.  **Analytical Question:** Do you see any signes of overfitting? Why? If it overfits, how would you fix this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770abf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0358745a",
   "metadata": {},
   "source": [
    "**Answer to Discussion Question:**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
